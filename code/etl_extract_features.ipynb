{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 17:48:18.219170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 17:48:18.232026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 17:48:18.232765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.config import list_physical_devices\n",
    "\n",
    "project_base_dir = 'big-data-project/'\n",
    "train_data_dir = os.path.join(project_base_dir, \"data\", \"train\")\n",
    "test_data_dir = os.path.join(project_base_dir, \"data\", \"test\")\n",
    "img_width = 1024\n",
    "img_height = 1024\n",
    "channel = 3\n",
    "num_classes= 2 #[pneumonia, normal] [bacterial, viral]\n",
    "\n",
    "print(\"Num GPUs Available: \", len(list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/09 17:48:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-12-09 17:48:21.731497: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-09 17:48:21.732519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 17:48:21.733296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 17:48:21.733960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 17:48:22.270286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 17:48:22.271040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 17:48:22.271740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 17:48:22.272374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13817 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1024, 1024, 3)]   0         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 32, 32, 2048)      23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d_la  (None, 2048)             0         \n",
      " st (GlobalAveragePooling2D)                                     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tensorflow import expand_dims\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "### Select the base model, here I have used a custom ResNet50 that was trained on detecting \"Normal\" vs \"Pneumonia\"\n",
    "base_model = applications.ResNet50(weights=None, include_top=False, input_shape=(img_width, img_height, channel))\n",
    "\n",
    "# Adding the top layer\n",
    "x = base_model(base_model.input, training=False)\n",
    "x = GlobalAveragePooling2D(name=\"global_average_pooling2d_last\")(x)\n",
    "predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "source_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Loading trained model for predicting \"Normal\" vs \"Pneumonia\"\n",
    "source_model.load_weights('big-data-project/weights_fine_tune/model_1.07-0.9644.hdf5') # change this to your path and model weights\n",
    "\n",
    "## Removing top dense layer and leaving just GlobalAveragePooling2D\n",
    "last_layer_name = \"global_average_pooling2d_last\"\n",
    "model = Model(inputs=source_model.input, outputs=source_model.get_layer(last_layer_name).output)\n",
    "model.summary()\n",
    "del source_model\n",
    "\n",
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content))\n",
    "    img = img.convert('RGB')\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_content(model, content):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = expand_dims(preprocess(content), axis=0)\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    return preds.flatten()\n",
    "\n",
    "def process_example(model, row):\n",
    "    features = featurize_content(model, row.content).tolist()\n",
    "    if \"NORMAL\" in row.path:\n",
    "        label = 0\n",
    "    if \"PNEUMONIA\" in row.path:\n",
    "        if \"bacteria\" in row.path.lower():\n",
    "            label = 2\n",
    "        elif \"virus\" in row.path.lower():\n",
    "            label = 1\n",
    "        else:\n",
    "            raise(\"Couldn't label image\")\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the base path to the *new* directory that will contain\n",
    "# our images after computing the training and testing split\n",
    "BASE_PATH = os.path.join(project_base_dir, \"dataset2\")\n",
    "\n",
    "# Define the names of the training, testing, and validation directories\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, \"train\")\n",
    "if not os.path.exists(TRAIN_PATH):\n",
    "    os.makedirs(TRAIN_PATH)\n",
    "\n",
    "TEST_PATH = os.path.join(BASE_PATH, \"test\")\n",
    "if not os.path.exists(TEST_PATH):\n",
    "    os.makedirs(TEST_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading training images from the mounted storage as binary files\n",
    "images_df = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.png\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 17:06:57.539930: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8200\n",
      "[Stage 280:>                                                        (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Writing features and labels for Training Set\n",
    "file = open(os.path.join(TRAIN_PATH, \"data_train_max_pool.csv\"), 'w', encoding=\"utf-8\")\n",
    "for row in images_df.rdd.collect:\n",
    "    features, label = process_example(model, row)\n",
    "    vec = \",\".join(map(str, features))\n",
    "    feature_row = f\"{label},{vec}\\n\"\n",
    "    file.write(feature_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading test images from the mounted storage as binary files\n",
    "images_df = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.png\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(test_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 17:48:39.246939: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8200\n",
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Writing features and labels for Test Set\n",
    "file = open(os.path.join(TEST_PATH, \"data_test_max_pool.csv\"), 'w', encoding=\"utf-8\")\n",
    "for row in images_df.rdd.collect:\n",
    "    features, label = process_example(model, row)\n",
    "    vec = \",\".join(map(str, features))\n",
    "    feature_row = f\"{label},{vec}\\n\"\n",
    "    file.write(feature_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
